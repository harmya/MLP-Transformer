{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 256\n",
    "batch_size = 64\n",
    "d_embed = 128\n",
    "num_heads = 8\n",
    "num_decoder_blocks = 6\n",
    "max_iterations = 4000\n",
    "learning_rate = 4e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_file('maze_runner.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He began his new life standing up, surrounded by cold darkness and stale, dusty air. Metal ground against metal; a lurching shudder shook the floor beneath him. He fell down at the sudden movement and shuffled backward on his hands and feet, drops of sweat beading on his forehead despite the cool air. His back struck a hard metal wall; he slid along it until he hit the corner of the room. Sinking to the floor, he pulled his legs up tight against his body, hoping his eyes would soon adjust to the\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "stringToInt = {ch : i for i,ch in enumerate(characters)}\n",
    "intToString = {i : ch for i,ch in enumerate(characters)}\n",
    "\n",
    "def encode_string(s):\n",
    "    return [stringToInt[ch] for ch in s]\n",
    "\n",
    "def decode_string(v):\n",
    "    return ''.join([intToString[i] for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode_string(text), dtype=torch.long)\n",
    "n = int(0.85*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i : i + context_length] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + context_length+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256]) torch.Size([64, 256])\n",
      "(tensor([54, 57, 57,  1, 46,  1, 53, 66, 59, 49]), tensor(68))\n",
      "ill a hundred feet away. The right wall was closin\n",
      "ll a hundred feet away. The right wall was closing\n"
     ]
    }
   ],
   "source": [
    "example_x, example_y = get_batch('train')\n",
    "print(example_x.shape, example_y.shape)\n",
    "print((example_x[0][:10], example_y[0][101]))\n",
    "print(decode_string(example_x[0][:50].tolist()))\n",
    "print(decode_string(example_y[0][:50].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_embed : int, vocab_size : int):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # this takes in a (batch_size, context_length) tensor and returns a (batch_size, context_length, d_embed) tensor \n",
    "        # it maps each token to its embedding\n",
    "        return self.embedding(x) * math.sqrt(self.d_embed)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_embed, context_length):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.context_length = context_length\n",
    "        # initialize the positional encoding of size (context_length, d_embed)\n",
    "        # for each position in context_length, we have a d_embed dimensional vector\n",
    "        positional_encoding = torch.zeros(context_length, d_embed)\n",
    "        # [0, 1, 2, 3, ..., context_length-1]\n",
    "        position_index = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # 10000^(2i/d_embed) where i is the dimension of the positional encoding\n",
    "        denominator = torch.exp(torch.arange(0, d_embed, 2).float() * (-math.log(10000.0) / d_embed))\n",
    "        \n",
    "        positional_encoding[ : , 0::2] = torch.sin(position_index * denominator)\n",
    "        positional_encoding[ : , 1::2] = torch.cos(position_index * denominator)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x + self.positional_encoding[: , :x.shape[1], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.mask = torch.tril(torch.ones(context_length, context_length))\n",
    "        # mask is a lower triangular matrix of shape (context_length, context_length)\n",
    "        # we use this to prevent the model from looking into the future\n",
    "        # for each position i, we set the mask to 0 for all positions j where j > i\n",
    "        # this way, the model can only attend to positions before i\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, sequence_length, feature_dimension = x.shape\n",
    "        K = self.key(x)\n",
    "        Q = self.query(x)\n",
    "        q_kt = Q @ K.transpose(-2, -1) / np.sqrt(feature_dimension) \n",
    "        q_kt = q_kt.masked_fill(self.mask[:sequence_length, :sequence_length] == 0, float('-inf'))\n",
    "        scaled_qkt = torch.nn.functional.softmax(q_kt, dim=-1)\n",
    "        V = self.value(x)\n",
    "\n",
    "        attention = scaled_qkt @ V\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for i in range(num_heads)])\n",
    "        self.linear_layer = nn.Linear(head_size * num_heads, d_embed) # head_size * num_heads = d_embed (usually)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.linear_layer(head_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_embed):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(d_embed, 4 * d_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_embed, d_embed)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, d_embed, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = d_embed // num_heads # head_size is \"how\" much of the embedding is \"seen\" by each head\n",
    "        self.multi_head_attention = MultiHeadAttention(head_size, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_embed)\n",
    "        self.feed_forward_layer = FeedForward(d_embed)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention = self.multi_head_attention(x)\n",
    "        x = self.layer_norm1(x + attention)\n",
    "        feed_forward = self.feed_forward_layer(x)\n",
    "        return self.layer_norm2(x + feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_embed, num_heads, num_blocks):\n",
    "        super().__init__()\n",
    "        self.input_embeddings = InputEmbeddings(d_embed, vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_embed, context_length)\n",
    "        self.blocks = nn.Sequential(*[Block(d_embed, num_heads) for i in range(num_blocks)])\n",
    "        self.final_layer_norm = nn.LayerNorm(d_embed)\n",
    "        self.output_layer = nn.Linear(d_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, x, y = None):\n",
    "        batch_size, sequence_length = x.shape\n",
    "        x_input_embeddings = self.input_embeddings(x)\n",
    "        x_positional = self.positional_encoding(x_input_embeddings)\n",
    "        block_out = self.blocks(x_positional)\n",
    "        layer_norm_out = self.final_layer_norm(block_out)\n",
    "        logits = self.output_layer(layer_norm_out)\n",
    "        loss = None\n",
    "        if y is None:\n",
    "            return logits, loss\n",
    "        else:\n",
    "            loss = nn.CrossEntropyLoss()(logits.view(-1, vocab_size), y.view(-1))\n",
    "            return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_length):\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_length):\n",
    "                context = x[:, -context_length:]\n",
    "                logits, _ = self.forward(context)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                x = torch.cat([x, next_token], dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_embed, num_heads, num_decoder_blocks)\n",
    "weights = torch.load('transformer.pth', map_location=device)\n",
    "model.load_state_dict(weights, strict=False)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_loss():\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        total_loss = 0\n",
    "        total_batches = 0\n",
    "        for i in range(100):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            total_loss += loss\n",
    "            total_batches += 1\n",
    "        print(f'{split} loss: {total_loss / total_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor epoch in range(1000):\\n    x, y = get_batch('train')\\n    x, y = x.to(device), y.to(device)\\n    logits, loss = model(x, y)\\n    optimizer.zero_grad()\\n    loss.backward()\\n    optimizer.step()\\n    if epoch % 100 == 0:\\n        print(f'epoch {epoch} loss: {loss}')\\n\""
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for epoch in range(1000):\n",
    "    x, y = get_batch('train')\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch} loss: {loss}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPie could fast. He becomently, how was impossibly, moving room do a bloody poles. Thomas hate tried before paused before away from how eyes. Everyoneds closer shut, and hearted for something. Alby—was a ’emp here.” He perfed to bosss’ hint he tugest to help, ready to get something as if many besing pularess. It was quickly with went? “All—was weent to mean above,” surprinted, Thomas shudded. “Aftered ustyed up, bran ask for things of us remember in. Just life memory had the Flll. Tomorrow, quiese the desturs.” Newt pulge shock rubbs upsit from behind and Thomas. “Greenbeally helf bane a lot hide falt him what?”  heard for silenate returning eath attached Thomas. “Why hore is remembered here memble.” Thomas reached the mornies—he’d low filled out the at Gally followed on out the right off invyines the night for as they stepped with lay well. He paused, put the commbing.  Newt felt secogy again. The watch of them. Thomas was louder, spikes. Only step of morning, he knew where’d never leg\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "# generate text\n",
    "x = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated = model.generate(x, 1000)\n",
    "g = decode(generated[0].tolist())\n",
    "print(g)\n",
    "print(len(g))\n",
    "# send to file\n",
    "with open('generated_text.txt', 'w') as f:\n",
    "    f.write(decode(generated[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
