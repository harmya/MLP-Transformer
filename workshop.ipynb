{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import re\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "batch_size = 8\n",
    "d_embed = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jokes(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        jokes = file.readlines()\n",
    "    jokes = [re.sub('\\n', '', joke) for joke in jokes]\n",
    "    return jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = load_jokes('jokes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_jokes = tokenizer(jokes, return_tensors='pt', padding=True, truncation=True, max_length=context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] im an avid supporter of the flat earth society i always have heated debate about it with my friend residing in the other hemisphere [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_TOKEN = tokenizer.cls_token_id\n",
    "END_TOKEN = tokenizer.sep_token_id\n",
    "PAD_TOKEN = tokenizer.pad_token_id\n",
    "t = tokenized_jokes['input_ids'][92]\n",
    "tokenizer.decode(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenized_jokes['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(jokes) * 0.85)\n",
    "train_jokes = data[:split_index]\n",
    "test_jokes = data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split_type=None):\n",
    "    jokes = train_jokes if split_type == 'train'or split_type == None else val_jokes\n",
    "    # get a offset between 0 and len(train_jokes) - batch_size - 1\n",
    "    random_idx = np.random.randint(0, len(jokes) - batch_size - 1)\n",
    "    x = torch.stack([jokes[random_idx + i] for i in range(0, batch_size)])\n",
    "    y = torch.stack([torch.cat((jokes[random_idx + i][1:], torch.tensor([PAD_TOKEN]))) for i in range(0, batch_size)])\n",
    "    return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128]) torch.Size([8, 128])\n",
      "tensor([  101,  2026,  3144,  6090, 17955,  2449,  2001,  3728,  3844,  2091,\n",
      "         2619, 11182,  2125,  1996,  2610,  2008,  1045,  2001,  4855,  2068,\n",
      "         2980,   102])\n",
      "tensor([ 2026,  3144,  6090, 17955,  2449,  2001,  3728,  3844,  2091,  2619,\n",
      "        11182,  2125,  1996,  2610,  2008,  1045,  2001,  4855,  2068,  2980,\n",
      "          102])\n",
      "\n",
      "tensor([  101,  2065,  3607, 18445,  2015,  4977,  2013,  1996,  4373,  2052,\n",
      "         5483,  2393,   102])\n",
      "tensor([ 2065,  3607, 18445,  2015,  4977,  2013,  1996,  4373,  2052,  5483,\n",
      "         2393,   102])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_x, example_y = get_batch('train')\n",
    "print(example_x.shape, example_y.shape)\n",
    "for i in range(2):\n",
    "    print(example_x[i][example_x[i] != PAD_TOKEN])\n",
    "    print(example_y[i][example_y[i] != PAD_TOKEN])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_embed : int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_embed)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_embed: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        positional_encoding = torch.zeros(d_embed, seq_len) # (d_embed, seq_len)\n",
    "        position_index = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        denominator = torch.exp(torch.arange(0, d_embed, 2).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n",
       "        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(0, d_embed, 2)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "input_embedding = InputEmbeddings(d_embed, vocab_size)\n",
    "positional_encoding = PositionalEncoding(d_embed, context_length, 0.1)\n",
    "\n",
    "x_example_embed = input_embedding(example_x)\n",
    "print(x_example_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(16, 10, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.mask = torch.tril(torch.ones(context_length, context_length))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length, feature_dimension = x.shape\n",
    "        K = self.key(x)\n",
    "        Q = self.query(x)\n",
    "        q_kt = Q @ K.transpose(-2, -1) / np.sqrt(feature_dimension) \n",
    "        q_kt = q_kt.masked_fill(self.mask == 0, float('-inf'))\n",
    "        similarity = torch.nn.functional.softmax(q_kt, dim=-1)\n",
    "        V = self.value(x)\n",
    "        attention = similarity @ V\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for i in range(num_heads)])\n",
    "        self.linear_layer = nn.Linear(head_size * num_heads, d_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.linear_layer(head_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, d_embed, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = d_embed // num_heads \n",
    "        self.multi_head_attention = MultiHeadAttention(head_size, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_embed)\n",
    "        self.feed_forward_layer = nn.Sequential(d_embed)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention = self.multi_head_attention(x)\n",
    "        x = self.layer_norm1(x + attention)\n",
    "        feed_forward = self.feed_forward_layer(x)\n",
    "        return self.layer_norm2(x + feed_forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
